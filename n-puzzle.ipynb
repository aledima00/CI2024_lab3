{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3 - N-puzzle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from icecream import ic\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the configuration.\n",
    "- change `PUZZLE_DIM` to change the size of the puzzle, considering that it represent the side of the square matrix\n",
    "- `RANDOMIZE_STEPS` is set to a value that is high enough for tested instances\n",
    "- other parameter are explained in the `README.md` and can be left like this or tweaked to test different configuration; note that to perform A* it is sufficient to put `HEURISTIC_WEIGHT=1` and `MAX_HEURISTIC_WEIGHT=1`\n",
    "- note that `HEURISTIC_WEIGHT_MSTEPS` is also used to print some statistics in such steps at runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUZZLE_DIM = 5\n",
    "RANDOMIZE_STEPS = 100_000\n",
    "\n",
    "HEURISTIC_WEIGHT = 1.5\n",
    "HEURISTIC_WEIGHT_MULT=1.05\n",
    "HEURISTIC_WEIGHT_MSTEPS = 10**PUZZLE_DIM\n",
    "MAX_HEURISTIC_WEIGHT = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i provide an easy configuration of the random number generator in order to make the pseudo-randomness controlled and easily reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.Generator(np.random.PCG64([PUZZLE_DIM, RANDOMIZE_STEPS]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the class `State`, that is used to represent a State in the search space.\n",
    "The class includes:\n",
    "- `state`: the actual state as a np array\n",
    "- `depth`: the current tree depth; if the current is the initial set, this is set to `0`;\n",
    "- `last_move`: the last move done used to going from parent state to current; if the current is the initial set, this is set to `None`;\n",
    "- `parent`: the parent state that generated the current state; if the current is the initial set, this is set to `None`;\n",
    "\n",
    "The class includes a set of methods useful for generation & initialization of state, actions evaluation and execution\n",
    "and cost computation.\n",
    "\n",
    "The cost is the sum of the actual cost and the heuristic cost function: in this case, i used **manhattan distance** as heuristic cost function, proven to be the best choice for path searching problems in grid-like environments where movement is restricted to horizontal and vertical directions (**sources**: *colleagues and lesson*, *https://theory.stanford.edu/~amitp/GameProgramming/Heuristics.html*,<br/> *https://pages.cs.wisc.edu/~dyer/cs540/notes/search2.html*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class State:\n",
    "\n",
    "    state: np.ndarray\n",
    "    depth: int\n",
    "    last_move: str\n",
    "    parent: \"State\"\n",
    "\n",
    "    def __init__(self, state: np.ndarray,parent: \"State\" = None,last_move:str=None):\n",
    "        \"\"\"\n",
    "        Initialize a new State object.\n",
    "        Parameters:\n",
    "            state (np.ndarray): The current state of the puzzle as a NumPy array.\n",
    "            last_move (str, optional): The last move made to reach the current state. Default is None.\n",
    "            parent (State, optional): The parent state from which the current state was generated. Default is None.\n",
    "        \"\"\"\n",
    "\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        if parent is not None:\n",
    "            self.depth = parent.depth+1\n",
    "            if last_move is None:\n",
    "                raise ValueError(\"Last move must be specified if parent is given\")\n",
    "            self.last_move = last_move\n",
    "        else:\n",
    "            self.depth = 0\n",
    "            self.last_move = None\n",
    "\n",
    "    @staticmethod\n",
    "    def getSolution():\n",
    "        \"\"\"\n",
    "        Generates the goal state for an n-puzzle game as a 2D numpy array with the numbers arranged in ascending order\n",
    "        from 1 to `PUZZLE_DIM**2 - 1`, with the last position being `0` (representing the empty space).\n",
    "        \"\"\"\n",
    "\n",
    "        return State(np.array([i for i in range(1, PUZZLE_DIM**2)]+[0]).reshape(PUZZLE_DIM, PUZZLE_DIM))\n",
    "\n",
    "    @staticmethod\n",
    "    def getRandom():\n",
    "        \"\"\"\n",
    "        Generate a random state for the n-puzzle problem.\n",
    "        \"\"\"\n",
    "        return State(rng.permutation(State.getSolution().state))\n",
    "    \n",
    "    def randomize(self, steps: int):\n",
    "        \"\"\"\n",
    "        Randomize the current state by applying `steps` random moves to it.\n",
    "        \"\"\"\n",
    "        for _ in tqdm(range(steps), desc=\"Randomizing\"):\n",
    "            action = rng.choice(self.available_actions())\n",
    "            self.state = self.do_action(action,as_child=False).state\n",
    "\n",
    "    def zeropos(self):\n",
    "        \"\"\"\n",
    "        Get the position of the zero (empty space) in the puzzle.\n",
    "        \"\"\"\n",
    "        for i in range(PUZZLE_DIM):\n",
    "            for j in range(PUZZLE_DIM):\n",
    "                if self.state[i,j] == 0:\n",
    "                    return i,j\n",
    "        raise ValueError(\"No zero found\")\n",
    "\n",
    "\n",
    "    def available_actions(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Get the list of available actions that can be taken in the current state, in format of [\"U\", \"D\", \"L\", \"R\"],\n",
    "        considering the action from the point of view of the empty tile (0).\n",
    "        \"\"\"\n",
    "        x, y = self.zeropos()\n",
    "        actions = list()\n",
    "        if x > 0:\n",
    "            actions.append(\"U\") \n",
    "        if x < PUZZLE_DIM - 1:\n",
    "            actions.append(\"D\")\n",
    "        if y > 0:\n",
    "            actions.append(\"L\")\n",
    "        if y < PUZZLE_DIM - 1:\n",
    "            actions.append(\"R\")\n",
    "        return actions\n",
    "    \n",
    "    def swap(self, pos1, pos2):\n",
    "        \"\"\"\n",
    "        Swap the positions of two elements in the puzzle.\n",
    "        \"\"\"\n",
    "        self.state[pos1], self.state[pos2] = self.state[pos2], self.state[pos1]\n",
    "\n",
    "    def do_action(self, action: str,*,as_child=True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform the given action on the current state. If `as_child` is `True`, the action is performed on a child state, otherwise it is performed on the current state.\n",
    "        Parameters:\n",
    "            action (str): The action to perform. Must be one of `\"U\"` (up), `\"D\"` (down), `\"L\"` (left), or `\"R\"` (right).\n",
    "            as_child (bool, optional): If `True`, create a new state as a child of the current state, and applies the action to it. If `False`, apply the action directly to the current state. Default is `True`.\n",
    "        Returns:\n",
    "            State: The state after applying the action.\n",
    "        \"\"\"\n",
    "\n",
    "        if as_child:\n",
    "            new_state = State(self.state.copy(),self,action)\n",
    "        else:\n",
    "            new_state = self # directly apply the action to the current state\n",
    "        \n",
    "        x, y = new_state.zeropos()\n",
    "        if action == \"U\":\n",
    "            new_state.swap((x, y), (x-1, y))\n",
    "        elif action == \"D\":\n",
    "            new_state.swap((x, y), (x+1, y))\n",
    "        elif action == \"L\":\n",
    "            new_state.swap((x, y), (x, y-1))\n",
    "        elif action == \"R\":\n",
    "            new_state.swap((x, y), (x, y+1))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "        return new_state\n",
    "    \n",
    "    def cost(self):\n",
    "        \"\"\"\n",
    "        Actual cost of the state in terms of the number of moves made to reach it.\n",
    "        \"\"\"\n",
    "        return self.depth\n",
    "    \n",
    "    def manhattan_distance(self):\n",
    "        \"\"\"\n",
    "        Calculate the Manhattan distance of the current state from the goal state.\n",
    "        \"\"\"\n",
    "        distance = 0\n",
    "        for i in range(PUZZLE_DIM):\n",
    "            for j in range(PUZZLE_DIM):\n",
    "                if self.state[i, j] == 0:\n",
    "                    continue # skip the hole\n",
    "                d, m = divmod(self.state[i, j] - 1, PUZZLE_DIM)\n",
    "                distance += abs(d - i) + abs(m - j)\n",
    "        return distance\n",
    "    \n",
    "    def tot_cost(self):\n",
    "        \"\"\"\n",
    "        Compute the total cost of the state, which is the sum of the actual cost and the heuristic cost, that in this case is the Manhattan distance.\n",
    "        Moreover, the heuristic cost is weighted by the factor `HEURISTIC_WEIGHT`.\n",
    "        Changing the value of `HEURISTIC_WEIGHT` will change the importance of the heuristic cost in the A* algorithm,\n",
    "        speeding up the search but potentially leading to suboptimal solutions.\n",
    "        \"\"\"\n",
    "        return self.cost() + HEURISTIC_WEIGHT*self.manhattan_distance()\n",
    "    \n",
    "    def is_solved(self):\n",
    "        \"\"\"\n",
    "        Check if the current state is the goal state.\n",
    "        \"\"\"\n",
    "        return np.array_equal(self.state, State.getSolution().state)\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.state)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.state)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return np.array_equal(self.state, other.state)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.state.tobytes())\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        # necessary for priority queue, as it uses the < operator in each tuple item\n",
    "        return self.tot_cost() < other.tot_cost()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i also designed some utility functions in order to reconstruct the path and to print the solution in different formats, including a gif visualization executed by `save_seq_as_gif`.\n",
    "Regarding format, note that the multitute of function is because of the different point of views that actions can be viewed from, if considering the moving hole or its neighbor tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_path(state: State):\n",
    "    \"\"\" Reconstructs the path from the initial state to the given state from the hole POV \"\"\"\n",
    "    path = list()\n",
    "    if state is not None:\n",
    "        while state.last_move is not None:\n",
    "            path.append(state.last_move)\n",
    "            state = state.parent\n",
    "    return path[::-1] # reverse the path\n",
    "\n",
    "def hole_to_tile(haction:str):\n",
    "    \"\"\" Returns the opposite encoding, switching from moving the hole to moving the tile \"\"\"\n",
    "    if haction == \"U\":\n",
    "        return \"D\"\n",
    "    if haction == \"D\":\n",
    "        return \"U\"\n",
    "    if haction == \"L\":\n",
    "        return \"R\"\n",
    "    if haction == \"R\":\n",
    "        return \"L\"\n",
    "    raise ValueError(\"Invalid action\")\n",
    "\n",
    "def expand_encoding(act:str)->str:\n",
    "    \"\"\" Returns the full action string \"\"\"\n",
    "    if act == \"U\":\n",
    "        return \"Up\"\n",
    "    if act == \"D\":\n",
    "        return \"Down\"\n",
    "    if act == \"L\":\n",
    "        return \"Left\"\n",
    "    if act == \"R\":\n",
    "        return \"Right\"\n",
    "    raise ValueError(\"Invalid action\")\n",
    "\n",
    "def movingTilePos(parent_state:State,tile_action:str):\n",
    "    \"\"\" Returns the position of the tile before the movement \"\"\"\n",
    "    x,y = parent_state.zeropos()\n",
    "    if tile_action == \"U\":\n",
    "        return x+1,y\n",
    "    if tile_action == \"D\":\n",
    "        return x-1,y\n",
    "    if tile_action == \"L\":\n",
    "        return x,y+1\n",
    "    if tile_action == \"R\":\n",
    "        return x,y-1\n",
    "    raise ValueError(\"Invalid action\")\n",
    "\n",
    "def explain_path(state: State):\n",
    "    \"\"\" Explains the path printing the actions the user would do (in tile encoding) \"\"\"\n",
    "    path = list()\n",
    "    if state is not None:\n",
    "        while state.last_move is not None:\n",
    "            tile_action = hole_to_tile(state.last_move)\n",
    "            state = state.parent\n",
    "            tpos = movingTilePos(state,tile_action)\n",
    "            path.append((tile_action,tpos))\n",
    "    path = path[::-1] # reverse the path\n",
    "    for action, pos in path:\n",
    "        print(f\"Move tile \\\"{state.state[pos]}\\\" {expand_encoding(action)}\")\n",
    "\n",
    "def save_seq_as_gif(state:State, output_filename:str,*, frame_duration:int=500, end_pause_duration:int=2000):\n",
    "    \"\"\" Visualizes the path in a gif; gifs can be parametrized with proper kwargs \"\"\"\n",
    "    states = list()\n",
    "    while state is not None:\n",
    "        states.append(state.state)\n",
    "        state = state.parent\n",
    "    states = states[::-1]\n",
    "\n",
    "    images = []\n",
    "    fig, ax = plt.subplots(figsize=(PUZZLE_DIM, PUZZLE_DIM))\n",
    "    ax.axis(\"off\")\n",
    "    for st in states:\n",
    "        ax.clear()\n",
    "\n",
    "        ax.set_xticks(np.arange(-0.5, PUZZLE_DIM, 1), minor=False)\n",
    "        ax.set_yticks(np.arange(-0.5, PUZZLE_DIM, 1), minor=False)\n",
    "        ax.grid(color=\"black\", linestyle=\"-\", linewidth=2)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.invert_yaxis() # Invert the y-axis to make rows appear from top to bottom\n",
    "\n",
    "        for i in range(PUZZLE_DIM):\n",
    "            for j in range(PUZZLE_DIM):\n",
    "                if st[i, j] == 0:\n",
    "                    ax.add_patch(plt.Rectangle((j-0.5, i-0.5), 1, 1, color=\"black\"))\n",
    "                else:\n",
    "                    ax.text(j, i, str(st[i, j]), ha=\"center\", va=\"center\", fontsize=20)\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        img = np.frombuffer(fig.canvas.buffer_rgba(), dtype=\"uint8\")\n",
    "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n",
    "        img = img[:, :, :3]  # Extract only RGB channels if required\n",
    "        images.append(Image.fromarray(img))\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # add pause on last frame\n",
    "    end_pause_frames = int(end_pause_duration // frame_duration)\n",
    "    images.extend([images[-1]] * end_pause_frames)\n",
    "    \n",
    "    # save the gif\n",
    "    images[0].save(\n",
    "        output_filename,\n",
    "        save_all=True,\n",
    "        append_images=images[1:],\n",
    "        duration=frame_duration,\n",
    "        loop=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to create a randomized solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e878d57432b840998064a476be48575d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Randomizing:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| state: [[23 22 21  8  3]\n",
      "            [19  0 18 13 15]\n",
      "            [ 6 11 12  9  1]\n",
      "            [24  4 17 20 14]\n",
      "            [10  5 16  7  2]]\n",
      "ic| state.manhattan_distance(): np.int64(82)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "state = State.getSolution()\n",
    "state.randomize(RANDOMIZE_STEPS)\n",
    "ic(state)\n",
    "ic(state.manhattan_distance());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A* Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's design the core of the __A* algorothm__.\n",
    "\n",
    "As first step, let's define the class `InvPriorityQueue` to implement an inverse priority queue using `heapq` package internally.\n",
    "Also the `StatePriorityQueue` is designed as inherited class of the former specifically designed to append states using as inverse priority their total cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "class InvPriorityQueue:\n",
    "    def __init__(self):\n",
    "        self.elements = []\n",
    "    \n",
    "    def empty(self):\n",
    "        return len(self.elements) == 0\n",
    "    \n",
    "    def append(self, item, inv_priority):\n",
    "        heapq.heappush(self.elements, (inv_priority, item))\n",
    "    \n",
    "    def popleft(self):\n",
    "        return heapq.heappop(self.elements)[1]\n",
    "    \n",
    "    def __bool__(self):\n",
    "        \"\"\" Override the boolean operator to allow direct check with while and if statements \"\"\"\n",
    "        return not self.empty()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.elements)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.elements)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.elements)\n",
    "    \n",
    "class StatePriorityQueue(InvPriorityQueue):\n",
    "    def append(self, item: State):\n",
    "        super().append(item, item.tot_cost())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can therefore generate the initial condition of the problem, using the previously defined variable `state`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| frontier.elements: [(np.float64(123.0),\n",
      "                         [[23 22 21  8  3]\n",
      "                        [19  0 18 13 15]\n",
      "                        [ 6 11 12  9  1]\n",
      "                        [24  4 17 20 14]\n",
      "                        [10  5 16  7  2]])]\n"
     ]
    }
   ],
   "source": [
    "frontier = StatePriorityQueue()\n",
    "frontier.append(state)\n",
    "visited = set()\n",
    "visited.add(state)\n",
    "ic(frontier.elements);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can run the core of the A* algorithm. The main loop is small, but it is enlarged by the added condition for the increase of heuristic weight and statistic print every `HEURISTIC_WEIGHT_MSTEPS` cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_state (@cycle-0):\n",
      "[[23 22 21  8  3]\n",
      " [19  0 18 13 15]\n",
      " [ 6 11 12  9  1]\n",
      " [24  4 17 20 14]\n",
      " [10  5 16  7  2]]\n",
      "current statistics:\n",
      "evalued states: 1\n",
      "frontier states: 4\n",
      "incrementing heuristic weight from 1.5 to 1.5750000000000002\n",
      "\n",
      "current_state (@cycle-100000):\n",
      "[[23 21  8  3 15]\n",
      " [ 6 22 13  9  1]\n",
      " [11 12 19  4 14]\n",
      " [16 10 17  5 20]\n",
      " [ 0 24  7 18  2]]\n",
      "current statistics:\n",
      "evalued states: 99546\n",
      "frontier states: 110699\n",
      "incrementing heuristic weight from 1.5750000000000002 to 1.6537500000000003\n",
      "\n",
      "current_state (@cycle-200000):\n",
      "[[ 6 23  8  1  3]\n",
      " [21  0 22  9  5]\n",
      " [11 12 13 14 15]\n",
      " [10  7 19  4 20]\n",
      " [16 17 18 24  2]]\n",
      "current statistics:\n",
      "evalued states: 197337\n",
      "frontier states: 250375\n",
      "incrementing heuristic weight from 1.6537500000000003 to 1.7364375000000003\n",
      "\n",
      "current_state (@cycle-300000):\n",
      "[[23 22 21  8  3]\n",
      " [ 6 12 19  9 15]\n",
      " [11 13  1  5 14]\n",
      " [16 10 18  2  4]\n",
      " [24  7 17  0 20]]\n",
      "current statistics:\n",
      "evalued states: 295682\n",
      "frontier states: 371888\n",
      "incrementing heuristic weight from 1.7364375000000003 to 1.8232593750000003\n",
      "\n",
      "current_state (@cycle-400000):\n",
      "[[23 21  1  3 15]\n",
      " [ 6 22  0  8  9]\n",
      " [11 12 13  2 14]\n",
      " [16 17  7 19  5]\n",
      " [ 4 24 18 10 20]]\n",
      "current statistics:\n",
      "evalued states: 392910\n",
      "frontier states: 511186\n",
      "incrementing heuristic weight from 1.8232593750000003 to 1.9144223437500005\n",
      "\n",
      "current_state (@cycle-500000):\n",
      "[[23 22 13 21  3]\n",
      " [ 6 19  0  8  9]\n",
      " [11 12  1 14 15]\n",
      " [ 4 16 18 17 20]\n",
      " [24 10  5  7  2]]\n",
      "current statistics:\n",
      "evalued states: 490465\n",
      "frontier states: 649330\n",
      "incrementing heuristic weight from 1.9144223437500005 to 2.010143460937501\n",
      "\n",
      "current_state (@cycle-600000):\n",
      "[[ 6  1 23  9  3]\n",
      " [22  0  8  4 15]\n",
      " [21 12 19 13 14]\n",
      " [11  7 18 10 20]\n",
      " [16 24  5 17  2]]\n",
      "current statistics:\n",
      "evalued states: 587360\n",
      "frontier states: 777555\n",
      "incrementing heuristic weight from 2.010143460937501 to 2.110650633984376\n",
      "\n",
      "current_state (@cycle-700000):\n",
      "[[23 21  8  9  3]\n",
      " [ 6  0  1  5 15]\n",
      " [11 12 13 10 14]\n",
      " [16  7 22 19 20]\n",
      " [24 17 18  4  2]]\n",
      "current statistics:\n",
      "evalued states: 684403\n",
      "frontier states: 917197\n",
      "incrementing heuristic weight from 2.110650633984376 to 2.216183165683595\n",
      "\n",
      "current_state (@cycle-800000):\n",
      "[[ 6  1 23  9  3]\n",
      " [21 22  8  4 15]\n",
      " [16 11 13 14  5]\n",
      " [ 7  0 12 19 10]\n",
      " [24 17 18  2 20]]\n",
      "current statistics:\n",
      "evalued states: 781299\n",
      "frontier states: 1041550\n",
      "incrementing heuristic weight from 2.216183165683595 to 2.3269923239677746\n",
      "\n",
      "current_state (@cycle-900000):\n",
      "[[23 21  3 19  1]\n",
      " [ 6 22 13  8  9]\n",
      " [11 12  0 14 15]\n",
      " [16  7 18  4 20]\n",
      " [24 17  5 10  2]]\n",
      "current statistics:\n",
      "evalued states: 877885\n",
      "frontier states: 1182498\n",
      "incrementing heuristic weight from 2.3269923239677746 to 2.4433419401661634\n",
      "\n",
      "current_state (@cycle-1000000):\n",
      "[[23 21  1  8  3]\n",
      " [ 6 22  4  0 15]\n",
      " [11 12  9  5 14]\n",
      " [16 18 13 19 20]\n",
      " [24 10  7 17  2]]\n",
      "current statistics:\n",
      "evalued states: 974585\n",
      "frontier states: 1320310\n",
      "incrementing heuristic weight from 2.4433419401661634 to 2.5655090371744715\n",
      "\n",
      "current_state (@cycle-1100000):\n",
      "[[ 6  1 23  9  3]\n",
      " [ 0 22  8 14  5]\n",
      " [21 12 13 10 15]\n",
      " [11 17 18  4 20]\n",
      " [16  7 19 24  2]]\n",
      "current statistics:\n",
      "evalued states: 1071004\n",
      "frontier states: 1446751\n",
      "incrementing heuristic weight from 2.5655090371744715 to 2.6937844890331952\n",
      "\n",
      "current_state (@cycle-1200000):\n",
      "[[23  1 21  4  3]\n",
      " [ 6 22  9  8  5]\n",
      " [16 11 13 14 15]\n",
      " [ 0 19 18 17 20]\n",
      " [24 12 10  7  2]]\n",
      "current statistics:\n",
      "evalued states: 1167117\n",
      "frontier states: 1581156\n",
      "incrementing heuristic weight from 2.6937844890331952 to 2.828473713484855\n",
      "\n",
      "current_state (@cycle-1300000):\n",
      "[[23 21  8  9  3]\n",
      " [ 6  1 12  4 15]\n",
      " [11 22  0 13 14]\n",
      " [16  7 19 18 20]\n",
      " [24 17 10  5  2]]\n",
      "current statistics:\n",
      "evalued states: 1263766\n",
      "frontier states: 1725630\n",
      "incrementing heuristic weight from 2.828473713484855 to 2.969897399159098\n",
      "\n",
      "current_state (@cycle-1400000):\n",
      "[[23 21  8  1  3]\n",
      " [ 6 22  2 13 15]\n",
      " [11 12  9 10 14]\n",
      " [16  7 19  0  5]\n",
      " [ 4 17 18 24 20]]\n",
      "current statistics:\n",
      "evalued states: 1359931\n",
      "frontier states: 1859193\n",
      "incrementing heuristic weight from 2.969897399159098 to 3\n",
      "\n",
      "current_state (@cycle-1500000):\n",
      "[[23 21  8  1  3]\n",
      " [11  6 22  9 15]\n",
      " [12  0 13  5 14]\n",
      " [16  7 19 18 20]\n",
      " [ 4 17 24 10  2]]\n",
      "current statistics:\n",
      "evalued states: 1455766\n",
      "frontier states: 1980616\n",
      "heuristic weight is already at max value: 3\n",
      "\n",
      "current_state (@cycle-1600000):\n",
      "[[23 21  8  3 15]\n",
      " [ 6 22 13  0  1]\n",
      " [11 12 19  9  5]\n",
      " [16  7 18 14  4]\n",
      " [24 17  2 10 20]]\n",
      "current statistics:\n",
      "evalued states: 1552291\n",
      "frontier states: 2117327\n",
      "heuristic weight is already at max value: 3\n",
      "\n",
      "current_state (@cycle-1700000):\n",
      "[[23 21  9  1  3]\n",
      " [ 6 22  8  5 15]\n",
      " [11 12 13  4 10]\n",
      " [16 18 19 14 20]\n",
      " [ 7 17 24  0  2]]\n",
      "current statistics:\n",
      "evalued states: 1648232\n",
      "frontier states: 2250662\n",
      "heuristic weight is already at max value: 3\n",
      "\n",
      "current_state (@cycle-1800000):\n",
      "[[23 21 19  8  3]\n",
      " [ 6 22  0  9 15]\n",
      " [11 12 13  1 14]\n",
      " [16 17  2  7  5]\n",
      " [ 4 24 18 10 20]]\n",
      "current statistics:\n",
      "evalued states: 1744387\n",
      "frontier states: 2391497\n",
      "heuristic weight is already at max value: 3\n",
      "\n",
      "current_state (@cycle-1900000):\n",
      "[[23 21  8  9  3]\n",
      " [ 6 22 13  1 15]\n",
      " [ 0 11 19 10 14]\n",
      " [16  7 12  2  5]\n",
      " [24 17 18  4 20]]\n",
      "current statistics:\n",
      "evalued states: 1840377\n",
      "frontier states: 2527920\n",
      "heuristic weight is already at max value: 3\n",
      "\n",
      "current_state (@cycle-2000000):\n",
      "[[ 6 23  0  9 15]\n",
      " [21 22  3  8  5]\n",
      " [11 12 13  1 14]\n",
      " [16  7 18 19 20]\n",
      " [ 4 17  2 24 10]]\n",
      "current statistics:\n",
      "evalued states: 1936057\n",
      "frontier states: 2653996\n",
      "heuristic weight is already at max value: 3\n",
      "\n",
      "current_state (@cycle-2100000):\n",
      "[[23 21  3  5  9]\n",
      " [ 6 22 13  8 15]\n",
      " [11  1 10  0 14]\n",
      " [16 12 18 19 20]\n",
      " [24  7 17  4  2]]\n",
      "current statistics:\n",
      "evalued states: 2031323\n",
      "frontier states: 2782145\n",
      "heuristic weight is already at max value: 3\n",
      "\n",
      "current_state (@cycle-2200000):\n",
      "[[ 1 12  3  4  5]\n",
      " [ 6  7  8  9 10]\n",
      " [11 18 13  0 15]\n",
      " [16  2 23 14 19]\n",
      " [21 22 17 24 20]]\n",
      "current statistics:\n",
      "evalued states: 2127611\n",
      "frontier states: 2925128\n",
      "heuristic weight is already at max value: 3\n",
      "\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "cycle=0\n",
    "while frontier:\n",
    "    current_state = frontier.popleft()\n",
    "    visited.add(current_state)\n",
    "    if current_state.is_solved():\n",
    "        print(\"Solved!\")\n",
    "        break\n",
    "    actions = current_state.available_actions()\n",
    "    for action in actions:\n",
    "        new_state = current_state.do_action(action)\n",
    "        if new_state not in visited:\n",
    "            frontier.append(new_state)\n",
    "    \"\"\"\n",
    "    - changes in the heuristic weight and print the current state and statistics every HEURISTIC_WEIGHT_MSTEPS cycles\n",
    "    - printing statistics\n",
    "    \"\"\"\n",
    "    if cycle % HEURISTIC_WEIGHT_MSTEPS == 0:\n",
    "        print(f\"current_state (@cycle-{cycle}):\\n{current_state}\")\n",
    "        print(f\"current statistics:\\nevalued states: {len(visited)}\\nfrontier states: {len(frontier)}\")\n",
    "        if HEURISTIC_WEIGHT < MAX_HEURISTIC_WEIGHT:\n",
    "            old_heuristic_weight = HEURISTIC_WEIGHT\n",
    "            HEURISTIC_WEIGHT *= HEURISTIC_WEIGHT_MULT\n",
    "            if HEURISTIC_WEIGHT > MAX_HEURISTIC_WEIGHT:\n",
    "                HEURISTIC_WEIGHT = MAX_HEURISTIC_WEIGHT\n",
    "            print(f\"incrementing heuristic weight from {old_heuristic_weight} to {HEURISTIC_WEIGHT}\\n\")\n",
    "        else:\n",
    "            print(f\"heuristic weight is already at max value: {HEURISTIC_WEIGHT}\\n\")\n",
    "    cycle += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hprint(*args,**kwargs):\n",
    "    \"\"\"Equivalent to print, but highlighted.\"\"\"\n",
    "    print('\\x1b[1;30;42m' + args[0] + '\\x1b[0m',*args[1:],**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal has been reached:\n",
      "[[ 1  2  3  4  5]\n",
      " [ 6  7  8  9 10]\n",
      " [11 12 13 14 15]\n",
      " [16 17 18 19 20]\n",
      " [21 22 23 24  0]]\n",
      "\u001b[1;30;42mQUALITY: SOLUTION consists of 156 actions\u001b[0m\n",
      "\u001b[1;30;42mCOST: THE NUMBER OF EVALUATED STATES IS 2133252, SO THE NUMBER OF EVALUATED ACTIONS IS 2133251\u001b[0m\n",
      "This is the path to be followed by a user to reach the goal from the initial state:\n",
      "Move tile \"19\" Right\n",
      "Move tile \"6\" Up\n",
      "Move tile \"11\" Left\n",
      "Move tile \"12\" Left\n",
      "Move tile \"18\" Down\n",
      "Move tile \"13\" Left\n",
      "Move tile \"9\" Up\n",
      "Move tile \"1\" Left\n",
      "Move tile \"14\" Up\n",
      "Move tile \"20\" Right\n",
      "Move tile \"17\" Right\n",
      "Move tile \"4\" Right\n",
      "Move tile \"5\" Up\n",
      "Move tile \"16\" Left\n",
      "Move tile \"7\" Left\n",
      "Move tile \"20\" Down\n",
      "Move tile \"17\" Right\n",
      "Move tile \"4\" Right\n",
      "Move tile \"24\" Right\n",
      "Move tile \"10\" Up\n",
      "Move tile \"5\" Left\n",
      "Move tile \"16\" Left\n",
      "Move tile \"7\" Left\n",
      "Move tile \"20\" Down\n",
      "Move tile \"17\" Right\n",
      "Move tile \"4\" Right\n",
      "Move tile \"5\" Up\n",
      "Move tile \"16\" Left\n",
      "Move tile \"17\" Down\n",
      "Move tile \"12\" Down\n",
      "Move tile \"18\" Down\n",
      "Move tile \"0\" Right\n",
      "Move tile \"11\" Up\n",
      "Move tile \"12\" Left\n",
      "Move tile \"18\" Down\n",
      "Move tile \"21\" Down\n",
      "Move tile \"8\" Left\n",
      "Move tile \"13\" Up\n",
      "Move tile \"9\" Up\n",
      "Move tile \"20\" Up\n",
      "Move tile \"7\" Up\n",
      "Move tile \"16\" Right\n",
      "Move tile \"17\" Down\n",
      "Move tile \"12\" Down\n",
      "Move tile \"18\" Down\n",
      "Move tile \"13\" Left\n",
      "Move tile \"9\" Up\n",
      "Move tile \"20\" Up\n",
      "Move tile \"17\" Right\n",
      "Move tile \"12\" Down\n",
      "Move tile \"9\" Left\n",
      "Move tile \"1\" Left\n",
      "Move tile \"15\" Down\n",
      "Move tile \"13\" Right\n",
      "Move tile \"8\" Down\n",
      "Move tile \"21\" Right\n",
      "Move tile \"18\" Up\n",
      "Move tile \"12\" Up\n",
      "Move tile \"11\" Right\n",
      "Move tile \"0\" Down\n",
      "Move tile \"22\" Down\n",
      "Move tile \"23\" Right\n",
      "Move tile \"19\" Up\n",
      "Move tile \"0\" Left\n",
      "Move tile \"22\" Down\n",
      "Move tile \"21\" Left\n",
      "Move tile \"8\" Left\n",
      "Move tile \"3\" Left\n",
      "Move tile \"15\" Up\n",
      "Move tile \"13\" Right\n",
      "Move tile \"18\" Right\n",
      "Move tile \"21\" Down\n",
      "Move tile \"8\" Left\n",
      "Move tile \"13\" Up\n",
      "Move tile \"18\" Right\n",
      "Move tile \"0\" Right\n",
      "Move tile \"11\" Up\n",
      "Move tile \"4\" Up\n",
      "Move tile \"17\" Left\n",
      "Move tile \"16\" Up\n",
      "Move tile \"5\" Right\n",
      "Move tile \"4\" Down\n",
      "Move tile \"24\" Right\n",
      "Move tile \"6\" Down\n",
      "Move tile \"19\" Down\n",
      "Move tile \"23\" Down\n",
      "Move tile \"22\" Left\n",
      "Move tile \"0\" Up\n",
      "Move tile \"11\" Up\n",
      "Move tile \"6\" Right\n",
      "Move tile \"24\" Up\n",
      "Move tile \"10\" Up\n",
      "Move tile \"5\" Left\n",
      "Move tile \"16\" Left\n",
      "Move tile \"7\" Left\n",
      "Move tile \"2\" Left\n",
      "Move tile \"14\" Down\n",
      "Move tile \"20\" Right\n",
      "Move tile \"7\" Up\n",
      "Move tile \"16\" Right\n",
      "Move tile \"17\" Down\n",
      "Move tile \"4\" Right\n",
      "Move tile \"11\" Down\n",
      "Move tile \"12\" Left\n",
      "Move tile \"18\" Down\n",
      "Move tile \"13\" Left\n",
      "Move tile \"9\" Up\n",
      "Move tile \"20\" Up\n",
      "Move tile \"17\" Right\n",
      "Move tile \"12\" Down\n",
      "Move tile \"9\" Left\n",
      "Move tile \"20\" Up\n",
      "Move tile \"14\" Left\n",
      "Move tile \"1\" Down\n",
      "Move tile \"9\" Right\n",
      "Move tile \"13\" Down\n",
      "Move tile \"18\" Right\n",
      "Move tile \"12\" Up\n",
      "Move tile \"11\" Right\n",
      "Move tile \"0\" Down\n",
      "Move tile \"18\" Left\n",
      "Move tile \"13\" Left\n",
      "Move tile \"8\" Down\n",
      "Move tile \"21\" Right\n",
      "Move tile \"22\" Right\n",
      "Move tile \"0\" Up\n",
      "Move tile \"18\" Left\n",
      "Move tile \"21\" Down\n",
      "Move tile \"8\" Left\n",
      "Move tile \"13\" Up\n",
      "Move tile \"15\" Left\n",
      "Move tile \"1\" Up\n",
      "Move tile \"9\" Right\n",
      "Move tile \"12\" Right\n",
      "Move tile \"18\" Down\n",
      "Move tile \"0\" Right\n",
      "Move tile \"11\" Up\n",
      "Move tile \"4\" Up\n",
      "Move tile \"5\" Up\n",
      "Move tile \"16\" Left\n",
      "Move tile \"17\" Down\n",
      "Move tile \"4\" Right\n",
      "Move tile \"11\" Down\n",
      "Move tile \"12\" Left\n",
      "Move tile \"9\" Left\n",
      "Move tile \"1\" Left\n",
      "Move tile \"14\" Up\n",
      "Move tile \"2\" Up\n",
      "Move tile \"7\" Right\n",
      "Move tile \"16\" Right\n",
      "Move tile \"5\" Right\n",
      "Move tile \"4\" Down\n",
      "Move tile \"17\" Left\n",
      "Move tile \"16\" Up\n",
      "Move tile \"7\" Left\n",
      "Move tile \"2\" Left\n"
     ]
    }
   ],
   "source": [
    "print(\"goal has been reached:\")\n",
    "print(current_state)\n",
    "hprint(f\"QUALITY: SOLUTION consists of {current_state.cost()} actions\")\n",
    "hprint(f\"COST: THE NUMBER OF EVALUATED STATES IS {len(visited)}, SO THE NUMBER OF EVALUATED ACTIONS IS {len(visited)-1}\")\n",
    "print(\"This is the path to be followed by a user to reach the goal from the initial state:\")\n",
    "explain_path(current_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the following cell to output the solution as a gif in `.gifs/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PUZZLE_DIM <= 3:\n",
    "    frame_duration = 500\n",
    "    end_pause_duration = 2000\n",
    "else:\n",
    "    frame_duration = 500/(2**(PUZZLE_DIM-3))\n",
    "    end_pause_duration = 2000*(2**(PUZZLE_DIM-3))\n",
    "save_seq_as_gif(current_state,f\"./gifs/{PUZZLE_DIM}x{PUZZLE_DIM}-puzzle_solution.gif\",frame_duration=frame_duration, end_pause_duration=end_pause_duration);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
